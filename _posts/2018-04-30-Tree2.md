---
layout: post
title:  "树模型汇总: 二 随机森林 Adaboost"
date:   2018-05-2 22:14:54
categories: 机器学习
tags: 机器学习 随机森林 Adaboost
excerpt: 机器学习笔记。
mathjax: true
typora-root-url: ..
typora-copy-images-to: ..\inner_ref\2018-04-30-Tree
---

* content
{:toc}

### 随机森林

鉴于决策树容易过拟合的缺点，随机森林采用多个决策树的投票机制来改善决策树，我们假设随机森林使用了m棵决策树，那么就需要产生m个一定数量的样本集来训练每一棵树，如果用全样本去训练m棵决策树显然是不可取的，全样本训练忽视了局部样本的规律，对于模型的泛化能力是有害的 

随机森林是一种重要的基于Bagging的集成学习方法，可以用来做分类、回归等问题。 

随机森林包括**列采样和行采样**。随机森林流程：

> 1. 假如有N个样本，则**有放回**的随机选择N个样本(每次随机选择一个样本，然后返回继续选择)。这选择好了的N个样本用来训练一个决策树，作为决策树根节点处的样本。
> 2.  当每个样本有M个属性时，在决策树的每个节点需要分裂时，随机从这M个属性中选取出m个属性，满足条件m << M。然后从这m个属性中采用某种策略（比如说信息增益）来选择1个属性作为该节点的分裂属性。
> 3. 决策树形成过程中每个节点都要按照步骤2来分裂（很容易理解，如果下一次该节点选出来的那一个属性是刚刚其父节点分裂时用过的属性，则该节点已经达到了叶子节点，无须继续分裂了）。一直到不能够再分裂为止。注意整个决策树形成过程中没有进行剪枝。
> 4.  按照步骤1~3建立大量的决策树，这样就构成了随机森林了。

**注意：**采样之后的数据使用完全分裂的方式建立出决策树，这样决策树的某一个叶子节点要么是无法继续分裂的，要么里面的所有样本的都是指向的同一个分类。一般很多的决策树算法都一个重要的步骤——剪枝，但是这里不这样干，由于之前的两个随机采样的过程保证了随机性，所以就算**不剪枝**，也不会出现over-fitting。 

随机森林有许多优点：

> 1. 具有极高的准确率
> 2. 随机性的引入，使得随机森林不容易过拟合
> 3. 随机性的引入，使得随机森林有很好的抗噪声能力
> 4. 能处理很高维度的数据，并且不用做特征选择
> 5. 既能处理离散型数据，也能处理连续型数据，数据集无需规范化
> 6. 训练速度快，可以得到变量重要性排序
> 7. 容易实现并行化

随机森林的缺点：

> 1. 当随机森林中的决策树个数很多时，训练时需要的空间和时间会较大
> 2. 随机森林模型还有许多不好解释的地方，有点算个黑盒模型

### Adaboost

AdaBoost，是英文"Adaptive Boosting"（自适应增强）的缩写，由Yoav Freund和Robert Schapire在1995年提出。它的自适应在于：前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基本分类器。同时，在每一轮中加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数 

具体说来，整个Adaboost 迭代算法就3步：

> 1. 初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：1/N。
> 2. 训练弱分类器。具体训练过程中，如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它的权值就被降低；相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高。然后，权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。
> 3. 将各个训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。换言之，**误差率低的弱分类器在最终分类器中占的权重较大，否则较小。**

#### Adaboost算法流程

给定一个训练数据集T={(x1,y1), (x2,y2)…(xN,yN)}，其中实例![x \in \mathcal{X}](/inner_ref/2018-04-30-Tree/624cf12f420fb0f373cda9f7b216b2f3.png)，而实例空间![\mathcal{X} \subset \mathbb{R}^n](/inner_ref/2018-04-30-Tree/d01e9255365440ae709190fafc071951.png)，yi属于标记集合{-1,+1}，Adaboost的目的就是从训练数据中学习一系列弱分类器或基本分类器，然后将这些弱分类器组合成一个强分类器。

​    Adaboost的算法流程如下：

**步骤1.** 首先，初始化训练数据的权值分布。每一个训练样本最开始时都被赋予相同的权值：1/N 

![img](/inner_ref/2018-04-30-Tree/20141102234630160) 

**步骤2.** 进行多轮迭代，用m = 1,2, ..., M表示迭代的第多少轮

​	**a**. 使用具有权值分布Dm的训练数据集学习，得到基本分类器（选取让误差率最低的阈值来设计基本分类器）： 

​	![img](/inner_ref/2018-04-30-Tree/20141102234909561) 

​	**b**. 计算Gm(x)在训练数据集上的分类误差率 

​	![img](/inner_ref/2018-04-30-Tree/20141102235141318) 

​	由上述式子可知，Gm(x)在训练数据集上的**误差率**em就是被Gm(x)误分类样本的权值之和。 

​	**c**. 计算Gm(x)的系数，$\alpha_m$表示Gm(x)在最终分类器中的重要程度（目的：得到基本分类器在最终分类器中所占的权重）： 

​	![img](/inner_ref/2018-04-30-Tree/20141102235307399) 

​	由上述式子可知，em <= 1/2时，am >= 0，且am随着em的减小而增大，意味着分类误差率越小的基本分类器在最终分类器中的作用越大。 

​	**d**. 更新训练数据集的权值分布（目的：得到样本的新的权值分布），用于下一轮迭代 

​	![img](/inner_ref/2018-04-30-Tree/20141103000618960) 

​	使得被基本分类器Gm(x)误分类样本的权值增大，而被正确分类样本的权值减小。就这样，通过这样的方式，AdaBoost方法能“重点关注”或“聚焦于”那些较难分的样本上。

​	其中，Zm是规范化因子，使得Dm+1成为一个概率分布：

​	![img](/inner_ref/2018-04-30-Tree/20141103000759596) 

**步骤3.** 组合各个弱分类器

![img](https://img-blog.csdn.net/20141103001101875) 

从而得到最终分类器，如下： 

![img](/inner_ref/2018-04-30-Tree/20141103001155359) 

### 参考资料

随机森林算法学习(RandomForest)： https://blog.csdn.net/qq547276542/article/details/78304454

说说随机森林： https://zhuanlan.zhihu.com/p/22097796