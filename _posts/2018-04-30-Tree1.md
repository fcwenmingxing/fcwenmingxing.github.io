---
layout: post
title:  "树模型汇总: 一 决策树"
date:   2018-04-30 15:14:54
categories: 机器学习
tags: 机器学习 决策树
excerpt: 机器学习笔记。
mathjax: true
typora-root-url: ..
typora-copy-images-to: ..\inner_ref

---

* content
{:toc}



### 概述

一直想把树模型给详细整理出来，让自己和大家都能够很好的理解它的“前生今世”。项目中经常用到gbdt做分类，Lambdamart做排序，但是里面的很多理论还不是太系统清楚，有些看过了又忘掉了，借这次机会想把它们都梳理清楚，相当于对自己做机器学习的一个测评。

### 决策树

决策树既可以解决分类问题，也可以解决回归问题。比较经典的决策树算法主要有ID3，C4.5 , CART。**其中ID3，C4.5属于分类树用于解决分类问题，CART既可以用于分类也可以用于回归。**在介绍它们之前先介绍几个会用到的公式。这几个公式决定了如何在生成树的时候选择最优划分

**熵：**熵越大，则随机变量的不确定性越大，对应于分类问题，即类别越混乱

$entropy(D) = -\sum_{i=1}^n P_i*log_2 P_i$

​	其中D为数据集，$i$为数据集D的可能分类标签,$P_i$为该标签的概率

**条件熵：** $entropy(D,A) = \sum_{i=1}^k \frac {D_{A_i}}{D}entropy(D_{A_i}) = \sum_{i=1}^k\frac{D_{A_i}}{D}\sum_{j=1}^nP_j*log_2P_j$

​	其中A表示约束特征，k表示A特征的种类。**tips: 条件熵表示在某个分类下的熵的概率之和**

**信息增益:** $gain(D,A) = entropy(D) - entropy(D,A)$

​	**tips：信息增益对可取值数目较多的属性有所偏好， 为减少这种偏好可能带来的不利影响。C4.5使用增益率来选择最优划分属性。**

**信息增益率：** $gain_{rate}(D,A) = gain(D,A)/entropy(D,A)$

​	**tips：增益率对可能取值数目较少的属性有所偏好**

**基尼指数:** $gini(D) = \sum_{k=1}^n p_k *(1-p_k) = \sum_{k=1}^n(p_k-p_k^2) = 1-\sum_{k=1}^np_k^2$

​	其中D为数据集，$k$为数据集D的可能分类标签,$P_k$为该标签的概率

**条件基尼指数：**$gini(D,A) = \sum_{i=1}^k \frac {D_{A_i}}{D} gini(D_{A_i})$

#### ID3算法

​	以原始数据集作为决策树的根节点，应用**信息增益**准则选择特征，将当前的数据集划分为**多个子集**，作为下一层的节点，在递归地处理这些节点，直到每个节点都是空集或者只包含一个类别的样本

![1525184150012](/inner_ref/2018-04-30-Tree/1525184150012.png)

![1525184176626](/inner_ref/2018-04-30-Tree/1525184176626.png)

​	上面的图就是在西瓜数据集上用ID3算法基于信息增益生成的决策树。它具有以下特点：

> 1. ID3算法采用了**信息增益**的方式做划分选择。仔细分析一下会发现一个问题：我们需要找到$gain(D,A)$最大的特征，对于一个数据集$entropy(D)$是给定的，也就是说我们需要$entropy(D,A)$最小，意思就是我们所选的特征是那些分完后子节点的纯度最高的特征，什么样的特征分完后子节点的特征纯度比较高(熵比较小)，该特征的子类别很多，即可取值很多的这一类特征。**总结一下就是信息增益偏向于去那些拥有很多子类的特征。**这是它致命的缺点
> 2. 它只能处理那些分类的特征，对于连续值特征毫无办法（其实我们可以人为的把连续属性给离散化，但是人为必然会导致可能不准确）。
> 3. 缺失值处理：需要人工处理。比如需要单独赋值或赋一个平均值

#### C4.5 算法

​	C4.5算法是ID3算法的改进，主要改进点就是解决了ID3的几个缺点。因此它跟 ID3 算法很相似。它具有以下特点：

> 1.  采用**信息增益率**来代替ID3中的信息增益。我们提到信息增益对可取值数目较多的属性有所偏好，但是增益率对可能取值数目较少的属性有所偏好。因此，C4.5并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式选择方式：**先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。**
> 2. C4.5采用了二分法（bi-partition）处理连续特征。1）对于连续值属性的值进行排序(A1,A2......An);2）在每两个值之间取一个点，用这个点就可以把该组连续值分为两部分，将它作为一个分裂点。为了找到最优分裂点我们需要遍历所有分裂点，因此它的时间开销很大。
> 3. 对于缺失值处理C4.5给出了一个很优雅的方式：C4.5让同一个样本以不同概率划入到不同的子节点中去。

缺失值处理方式，简单来说就是给每个样本赋予一个权重，初始为1。当在进行划分属性选择的时候只使用在该属性上没有缺失的样本。在将样本划分到子节点时，根据不同属性值的概率放入子节点中

![1525186548382](/inner_ref/2018-04-30-Tree/1525186548382.png)

![1525186574830](/inner_ref/2018-04-30-Tree/1525186574830.png)

#### CART树

CART树全称：Classification and Regression Trees。从名字就可以看出分类和回归任务它都可以做。这个树非常出名，我们以后使用到的random forest，gbdt， xgboost里面的base estimator 都是CART树。

CART算法由以下两步组成：

> 1. **决策树生成**：基于训练数据集生成决策树，生成的决策树要尽量大；
> 2. **决策树剪枝**：用验证数据集对已生成的树进行剪枝并选择最优子树，这时损失函数最小作为剪枝的标准。

##### 分类问题

对于分类问题它跟ID3，C4.5基本差不多。主要有两个区别：

> 1. 使用基尼指数来确定最优划分点。基尼指数的通俗解释就是：表示一件事物的不确定性，基尼指数越大不确定性越大。我们要找基尼指数小的特征，这样的特征对于划分数据集的准确性会更高(不确定性低嘛)
> 2. 它是一个二叉树，内部结点特征的取值为“是”和“否”，左分支是取值为“是”的分支，右分支是取值为“否”的分支。（对于连续特征，选择某个使得基尼指数最小的划分点；对于离散特征，对每个特征取值$a$用是否  $x_A=a$来划分）

##### 回归问题

CART做回归用平方误差表示每个单元的损失，：$\sum_{x_i \in R_m}(y_i-f(x_i))^2$，**每个单元的最优输出就是使该单元的损失函数最小。**每个单元的最终输出可以表示为$C = avg(y_i\|x_i)(x_i \in R_m)$(区间$R_m$ 上所有$x_i$ 的输出$y_i$的均值)

对于回归问题，我们面临的问题也是如何确定划分点(决策树的核心)。这里CART树的处理方式和C4.5处理连续变量的方式有点类似，即对于每个特征的取值，我们从中找一个点j，这个点j可以将该特征分为左右两部分。在划分选择时，选择满足左右子树平方误差之和最小的分割点：

![img](/inner_ref/2018-04-30-Tree/f33c7d4396da438d91e0b79c3f130ab8.jpg)

#### 说明

在使用决策树寻找每一步的最优切分点时，常用的是贪心算法，贪心算法有一个问题就是局部最优，而不是全局最优（因为从函数空间里所有的决策树中找出最优的决策树是NP-C问题 ，所以常采用启发式（Heuristic）的方法，如CART里面的优化GINI指数、剪枝、控制树的深度。 ）。所以我们一定要记住，**决策树在选择特征及切分点时考虑的是一个局部最优问题。**

#### 剪枝

#### 总结

在GBDT的迭代中，假设我们前一轮迭代得到的强学习器是$f_{t-1}(x)$, 损失函数是$L(y, f_{t-1}(x))$, 我们本轮迭代的目标是找到一个CART回归树模型的弱学习器$h_t(x)$，让本轮的损失损失$L(y, f_{t}(x) =L(y, f_{t-1}(x)+ h_t(x))$最小。也就是说，本轮迭代找到决策树，要让样本的损失尽量变得更小。 

对决策树ID3，C4.5， CART树做一个总结，便于记忆

| 类型   | 使用范围   | 特征选择方式                               | 树形   | 缺失值                     | 连续特征       |
| ------ | ---------- | ------------------------------------------ | ------ | -------------------------- | -------------- |
| ID3    | 分类       | 信息增益                                   | N叉树  | 人工处理                   | 不支持         |
| C4.5   | 分类       | 信息增益率                                 | N叉树  | 以不同概率<br>划入不同节点 | 支持（二分法） |
| CART树 | 分类和回归 | 分类：基尼指数(速度更快)<br>回归：平方误差 | 二叉树 |                            | 支持（二分法） |


### 小结之承上启下

#### Bagging和Boosting 概念

Bagging和Boosting都是将已有的分类或回归算法通过一定方式组合起来，形成一个性能更加强大的分类器，更准确的说这是一种分类算法的组装方法。即将弱分类器组装成强分类器的方法。

首先介绍Bootstraping，即自助法：它是一种有放回的抽样方法（可能抽到重复的样本）。

##### Bagging

Bagging (bootstrap aggregating)即套袋法，其算法过程如下：

> 1. 从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）
> 2. 每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）
> 3. 对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）

##### Boosting

其主要思想是将弱分类器组装成一个强分类器。在PAC（概率近似正确）学习框架下，则一定可以将弱分类器组装成一个强分类器。

关于Boosting的两个核心问题：

> 1. 在每一轮如何改变训练数据的权值或概率分布？
>
>    通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。
>
> 2. 通过什么方式来组合弱分类器？
>
>    通过加法模型将弱分类器进行线性组合，比如AdaBoost通过加权多数表决的方式，即增大错误率小的分类器的权值，同时减小错误率较大的分类器的权值。而提升树通过拟合残差的方式逐步减小残差，将每一步生成的模型叠加得到最终模型。

##### Bagging和Boosting的区别

> 1. 样本选择上：
>
>    Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。
>
>    Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。
>
> 2. 样例权重：
>
>   Bagging：使用均匀取样，每个样例的权重相等
>
>   Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。
>
> 3. 预测函数：
>
>   Bagging：所有预测函数的权重相等。
>
>   Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。
>
> 4. 并行计算：
>
>   Bagging：各个预测函数可以并行生成
>
>   Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。

##### 总结

这两种方法都是把若干个分类器整合为一个分类器的方法，只是整合的方式不一样，最终得到不一样的效果，将不同的分类算法套入到此类算法框架中一定程度上会提高了原单一分类器的分类效果，但是也增大了计算量。

下面是将决策树与这些**算法框架**进行结合所得到的新的算法：

1）Bagging + 决策树 = 随机森林

2）AdaBoost + 决策树 = 提升树

3）Gradient Boosting + 决策树 = GBDT

### 参考资料

决策树：https://www.cnblogs.com/wenyi1992/p/7685131.html

随机森林算法学习(RandomForest)： https://blog.csdn.net/qq547276542/article/details/78304454

说说随机森林： https://zhuanlan.zhihu.com/p/22097796