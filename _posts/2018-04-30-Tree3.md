---
layout: post
title:  "树模型汇总: 三 GBDT XGBoost"
date:   2018-05-2 22:14:54
categories: 机器学习
tags: 机器学习 GBDT XGBoost
excerpt: 机器学习笔记。
mathjax: true
typora-root-url: ..
typora-copy-images-to: ..\inner_ref

---

* content
{:toc}

### GBDT

GBDT有很多简称，有GBT（Gradient Boosting Tree）, GTB（Gradient Tree Boosting ）， GBRT（Gradient Boosting Regression Tree）, MART(Multiple Additive Regression Tree)，其实都是指的同一种算法。一般叫的比较多的是GBDT或MART（比如， LambdaMart中的Mart就是指的GBDT）。GBDT在业内有非常广泛的应用。它不需要对特征做归一化，而且能够自动做特征组合，可以得到特征的重要度。

GBDT分为两个版本：**残差版本**和**梯度版本**。这两个版本的思想有较明显区别。本文分开去讨论。

#### 残差版本

残差其实就是真实值和预测值之间的差值，在学习的过程中，首先学习一颗回归树，然后将“真实值-预测值”得到残差，再把残差作为一个学习目标，学习下一棵回归树，依次类推，直到残差小于某个接近0的阀值或回归树数目达到某一阀值。其核心思想是每轮通过拟合残差来降低损失函数。总的来说，第一棵树是正常的，之后所有的树的决策全是由残差来决定。 残差版本其实比较通俗易理解，而且经常在很多教材中提及。

它的一个简单流程：

我们先用一个初始值来学习一棵决策树，叶子处可以得到预测的值，以及预测之后的残差，然后后面的决策树就要基于前面决策树的残差来学习，直到预测值和真实值的残差为零。最后对于测试样本的预测值，就是前面许多棵决策树预测值的累加。 

该流程比较简单，而且通俗易懂。需要注意几点：

> 1. 这个版本的核心思路：每个回归树学习前面树的残差，并且用shrinkage把学习到的结果大步变小步，不断迭代学习。其中的代价函数是常见的**均方差。**
> 2. 其基本做法就是：先学习一个回归树，然后“真实值-预测值*shrinkage”求此时的残差，把这个残差作为目标值，学习下一个回归树，继续求残差……直到建立的回归树的数目达到一定要求或者残差能够容忍，停止学习。
> 3. 我们知道，残差是预测值和目标值的差值，这个版本是把残差作为全局最优的绝对方向来学习。
> 4. 这个版本更加适用于回归问题，线性和非线性的均可，而且在设定了阈值之后还可以有分类的功能。
> 5. **当时该版本使用残差，很难处理纯回归以外的问题。版本二中使用梯度，只要建立的代价函数能够求导，那么就可以使用版本二的GBDT算法，例如LambdaMART学习排序算法。**
> 6. Shrinkage和梯度下降法中学习步长alpha的关系。shrinkage设小了只会让学习更慢，设大了就等于没设，它适用于所有增量迭代求解问题；而Gradient的步长设小了容易陷入局部最优点，设大了容易不收敛。它仅用于用梯度下降求解。这两者其实没太大关系。

#### 梯度版本 

与残差版本把GBDT说成一个残差迭代树，认为每一棵回归树都在学习前N-1棵树的残差不同，Gradient版本把GBDT说成一个梯度迭代树，使用梯度下降法求解，认为每一棵回归树在学习前N-1棵树的梯度下降值。总的来说两者相同之处在于，都是迭代回归树，都是累加每颗树结果作为最终结果（Multiple Additive Regression Tree)，每棵树都在学习前N-1棵树尚存的不足，从总体流程和输入输出上两者是没有区别的。

两者的不同主要在于每步迭代时，是否使用Gradient作为求解方法。前者不用Gradient而是用残差—-残差是全局最优值，Gradient是局部最优方向*步长，即前者每一步都在试图让结果变成最好，后者则每步试图让结果更好一点。

两者优缺点。看起来前者更科学一点–有绝对最优方向不学，为什么舍近求远去估计一个局部最优方向呢？原因在于灵活性。**前者最大问题是，由于它依赖残差，cost function一般固定为反映残差的均方差，因此很难处理纯回归问题之外的问题。而后者求解方法为梯度下降，只要可求导的cost function都可以使用。**  跟残差版本比，梯度版本相对难理解，但是使用范围更广，非常适合跟其他模型组合，比如我们后续会讲到的LambdaMart。

**定义：**GBDT使用CART回归树模型，在迭代中，假设我们前一轮迭代得到的强学习器是$f_{t-1}(x)$, 损失函数是$L(y, f_{t-1}(x))$, 我们本轮迭代的目标是找到一个CART回归树模型的弱学习器$h_t(x)$，让本轮的损失损失$L(y, f_{t}(x)) =L(y, f_{t-1}(x)+h_t(x))$最小。也就是说，本轮迭代找到决策树，要让样本的损失尽量变得更小。 

##### GBDT的负梯度拟合（每棵树的真值）

我们介绍了GBDT的基本思路，但是没有解决损失函数拟合方法的问题。针对这个问题，大牛Freidman提出了用损失函数的负梯度来拟合本轮损失的近似值，进而拟合一个CART回归树。**第t轮的第i个样本**的损失函数的负梯度表示为

$r_{ti} = -\bigg[\frac{\partial L(y_i, f(x_i)))}{\partial f(x_i)}\bigg]_{f(x) = f_{t-1}\;\; (x)}$

利用$(x_i,r_{ti})\;\; (i=1,2,..m)​$,我们可以拟合一颗CART回归树，得到了第t颗回归树，其对应的叶节点区域$R_{tj}, j =1,2,..., J​$。其中$J​$为叶子节点的个数。（**tips：负梯度也就是每棵树拟合时的真值**）

针对每一个叶子节点里的样本，我们求出使损失函数最小，也就是拟合叶子节点最好的的输出值$c_{tj}$如下：

$c_{tj} = \underbrace{arg\; min}_{c}\sum\limits_{x_i \in R_{tj}} L(y_i,f_{t-1}(x_i) +c)$

如果损失函数式平方误差 $c_{tj}=\frac{\sum\limits_{{x_i\in R_{tj}}}(y_i-f_{t-1}(x_i))}{n}$, 也就是残差的均值

这样我们就得到了本轮的决策树拟合函数如下：

$h_t(x) = \sum\limits_{j=1}^{J}c_{tj}I(x \in R_{tj})$

从而本轮最终得到的强学习器的表达式如下：

$f_{t}(x) = f_{t-1}(x) + \sum\limits_{j=1}^{J}c_{tj}I(x \in R_{tj})$

通过损失函数的负梯度来拟合，**我们找到了一种通用的拟合损失误差的办法，这样无论是分类问题还是回归问题，我们通过其损失函数的负梯度的拟合，就可以用GBDT来解决我们的分类回归问题。区别仅仅在于损失函数不同导致的负梯度不同而已。**

GBDT常用的损失函数和对应的负梯度。

![img](/inner_ref/2018-04-30-Tree/517947-20170609154837559-1727480067.png) 

##### GBDT回归算法

好了，有了上面的思路，下面我们总结下GBDT的回归算法。为什么没有加上分类算法一起？那是因为分类算法的输出是不连续的类别值，需要一些处理才能使用负梯度，我们在下一节讲。 

输入是训练集样本$T=\{(x_,y_1),(x_2,y_2), ...(x_m,y_m)\}$， 最大迭代次数T, 损失函数L。

　　　　输出是强学习器$f(x)$

　　　　1) 初始化弱学习器

​                       $f_0(x) = \underbrace{arg\; min}_{c}\sum\limits_{i=1}^{m}L(y_i, c)$

　　　　2) 对迭代轮数$t=1,2,...T$有：

　　　　　　a)对样本$i=1,2，...m$，计算负梯度

​                              $r_{ti} = -\bigg[\frac{\partial L(y_i, f(x_i)))}{\partial f(x_i)}\bigg]_{f(x) = f_{t-1}\;\; (x)}$

　　　　　　b)利用$(x_i,r_{ti})\;\; (i=1,2,..m)$, 拟合一颗CART回归树,得到第t颗回归树，其对应的叶子节点区域为$R_{tj}, j =1,2,..., J$。其中$J$为回归树t的叶子节点的个数。

　　　　　　c) 对叶子区域$j =1,2,..J,$计算最佳拟合值

​                             $c_{tj} = \underbrace{arg\; min}_{c}\sum\limits_{x_i \in R_{tj}} L(y_i,f_{t-1}(x_i) +c)$

　　　　　　d) 更新强学习器

​                           $f_{t}(x) = f_{t-1}(x) + \sum\limits_{j=1}^{J}c_{tj}I(x \in R_{tj})$

　　　　3) 得到强学习器$f(x)$的表达式

​                         $f(x) = f_T(x) =f_0(x) + \sum\limits_{t=1}^{T}\sum\limits_{j=1}^{J}c_{tj}I(x \in R_{tj})$

截图来自《The Elements of Statistical Learning》 

![img](/inner_ref/2018-04-30-Tree/517947-20170609155109106-1915188978.png) 

##### GBDT分类算法

这里我们再看看GBDT分类算法，GBDT的分类算法从思想上和GBDT的回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合类别输出的误差。

为了解决这个问题，主要有两个方法，一个是用指数损失函数，此时GBDT退化为Adaboost算法。另一种方法是用类似于逻辑回归的对数似然损失函数的方法。也就是说，我们用的是类别的预测概率值和真实概率值的差来拟合损失。本文仅讨论用对数似然损失函数的GBDT分类。而对于对数似然损失函数，我们又有二元分类和多元分类的区别。

**二元GBDT分类算法**

对于二元GBDT，如果用类似于逻辑回归的对数似然损失函数，则损失函数为：

$L(y, f(x)) = log(1+ exp(-yf(x)))$

其中$y \in\{-1, +1\}$。则此时的负梯度误差为

$r_{ti} = -\bigg[\frac{\partial L(y, f(x_i)))}{\partial f(x_i)}\bigg]_{f(x) = f_{t-1}\;\; (x)} = y_i/(1+exp(y_if(x_i)))$

对于生成的决策树，我们各个叶子节点的最佳残差拟合值为

$c_{tj} = \underbrace{arg\; min}_{c}\sum\limits_{x_i \in R_{tj}} log(1+exp(-y_i(f_{t-1}(x_i) +c)))$

由于上式比较难优化，我们一般使用近似值代替

$c_{tj} = \sum\limits_{x_i \in R_{tj}}r_{ti}\bigg /  \sum\limits_{x_i \in R_{tj}}|r_{ti}|(1-|r_{ti}|)$

除了负梯度计算和叶子节点的最佳残差拟合的线性搜索，二元GBDT分类和GBDT回归算法过程相同



**多元GBDT分类算法**

多元GBDT要比二元GBDT复杂一些，对应的是多元逻辑回归和二元逻辑回归的复杂度差别。假设类别数为K，则此时我们的对数似然损失函数为：

$L(y, f(x)) = -  \sum\limits_{k=1}^{K}y_klog\;p_k(x)$

其中如果样本输出类别为$k$，则$y_k=1$。第$k$类的概率$p_k(x)$的表达式为：

$p_k(x) = exp(f_k(x)) \bigg / \sum\limits_{l=1}^{K} exp(f_l(x))$

集合上两式，我们可以计算出第t轮的第i个样本对应类别l的负梯度误差为
$r_{til} = -\bigg[\frac{\partial L(y_i, f(x_i)))}{\partial f(x_i)}\bigg]_{f_k(x) = f_{l, t-1}\;\; (x)} = y_{il} - p_{l, t-1}(x_i)$
观察上式可以看出，其实这里的误差就是样本i对应类别l的真实概率和$t-1$轮预测概率的差值。

对于生成的决策树，我们各个叶子节点的最佳残差拟合值为

$c_{tjl} = \underbrace{arg\; min}_{c_{jl}}\sum\limits_{i=0}^{m}\sum\limits_{k=1}^{K} L(y_k, f_{t-1, l}(x) + \sum\limits_{j=0}^{J}c_{jl} I(x_i \in R_{tj}))$

由于上式比较难优化，我们一般使用近似值代替

$c_{tjl} =  \frac{K-1}{K} \; \frac{\sum\limits_{x_i \in R_{tjl}}r_{til}}{\sum\limits_{x_i \in R_{til}}|r_{til}|(1-|r_{til}|)}$

除了负梯度计算和叶子节点的最佳残差拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同。

#### XGBoost

严格来说XGBoost是一个工具，它是GBDT的一种实现，在基本GBDT基础上它做了很多的改进，使得GBDT发扬光大。XGBoost的一些特点(知乎)

> 1. 传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。（本人不是太了解）
> 2. 传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，**xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导**。 
> 3. **xgboost在代价函数里加入了正则项，用于控制模型的复杂度。**正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。
> 4. Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）
> 5. 列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。
> 6. 对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。 
> 7. xgboost工具支持并行。boosting不是一种串行的结构吗?怎么并行的？注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。
> 8. 可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。
> 9. xgboost工具支持并行。boosting不是一种串行的结构吗?怎么并行的？注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行
> 10. 可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。

本人觉得2、3点非常重要，2使得用户可以根据需求自定义损失函数，也可以方便的跟其他模型组合。3防止了模型过拟合，相当于做了预剪枝。

下面对陈天奇的slides做一些摘录。

XGBoost的损失函数（也叫目标函数）表示为：

![1525825181426](/inner_ref/2018-04-30-Tree/1525825181426.png)

损失函数分为两部分： 第一部分控制训练误差，它跟样本数有关，第二部分控制模型复杂度，它跟树的个数有关。



在t轮迭代中，我们的未知量为$f_t(x_i)$， $\hat{y}_i^{(t-1)}$是前t-1轮的结果，是已知量。目标函数可以转换为

![1525825323967](/inner_ref/2018-04-30-Tree/1525825323967.png)

我们针对一棵树做如下定义：

$q_{(x)}$表示样本x落在了第$q_{x}$个叶子节点上。

$T$表示叶子节点总数

叶子节点j上的样本结合表示为$I_j=\{i|q(x_i)=j\}$

$w_i$表示第i个叶子节点上的取值。所以对于一个样本有：$f_t(x) = w_{q(x)}$。(**这个跟GBDT中的$f_t(x)$含义不一样**)

![1525825976283](/inner_ref/2018-04-30-Tree/1525825976283.png)

控制模型复杂度的正则项，它包括两部分： 第一部分控制叶子节点的个数，第二部分控制叶子节点值的大小。

定义如下：

![1525826188455](/inner_ref/2018-04-30-Tree/1525826188455.png)

对目标函数做泰勒展开（g表示一阶导数，h表示二阶导数）

![1525826525569](/inner_ref/2018-04-30-Tree/1525826525569.png)

![1525826637811](/inner_ref/2018-04-30-Tree/1525826637811.png)

**上面的$w^*_j$表示叶子节点的取值。**

如何构建一颗树？ XGBoost采用了贪心算法， 寻找gain最大的分裂位置

![1525826756980](/inner_ref/2018-04-30-Tree/1525826756980.png)

![1525826888663](/inner_ref/2018-04-30-Tree/1525826888663.png)

### 参考资料

[梯度提升树(GBDT)原理小结](https://www.cnblogs.com/pinard/p/6140514.html)	

[GBDT的数学原理](https://www.cnblogs.com/ljy2013/p/6972426.html)

[机器学习算法中GBDT和XGBOOST的区别有哪些](https://www.zhihu.com/question/41354392)

[陈天奇slides](/inner_ref/2018-04-30-Tree/BoostedTree.pdf)

