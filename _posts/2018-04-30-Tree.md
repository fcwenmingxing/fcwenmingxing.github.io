---
layout: post
title:  "树模型汇总(从决策树到随机森林, adboost, gbdt, xgboost, Lambdamart)"
date:   2018-04-30 15:14:54
categories: 机器学习
tags: 机器学习 决策树 gbdt
excerpt: 机器学习笔记。
mathjax: true
typora-root-url: ..
typora-copy-images-to: ..\inner_ref
---

* content {:toc}

{:toc}

### 概述

一直想把树模型给详细整理出来，让自己和大家都能够很好的理解它的“前生今世”。项目中经常用到gbdt做分类，Lambdamart做排序，但是里面的很多理论还不是太系统清楚，有些看过了又忘掉了，借这次机会想把它们都梳理清楚，相当于对自己做机器学习的一个测评。

### 决策树

决策树既可以解决分类问题，也可以解决回归问题。比较经典的决策树算法主要有ID3，C4.5 , CART。**其中ID3，C4.5属于分类树用于解决分类问题，CART既可以用于分类也可以用于回归。**在介绍它们之前先介绍几个会用到的公式。这几个公式决定了如何在生成树的时候选择最优划分

**熵：**熵越大，则随机变量的不确定性越大，对应于分类问题，即类别越混乱

$entropy(D) = -\sum_{i=1}^n P_i*log_2 P_i$

​	其中D为数据集，$i$为数据集D的可能分类标签,$P_i$为该标签的概率

**条件熵：** $entropy(D,A) = \sum_{i=1}^k \frac {D_{A_i}}{D}entropy(D_{A_i}) = \sum_{i=1}^k\frac{D_{A_i}}{D}\sum_{j=1}^nP_j*log_2P_j$

​	其中A表示约束特征，k表示A特征的种类。**tips: 条件熵表示在某个分类下的熵的概率之和**

**信息增益:** $gain(D,A) = entropy(D) - entropy(D,A)$

​	**tips：信息增益对可取值数目较多的属性有所偏好， 为减少这种偏好可能带来的不利影响。C4.5使用增益率来选择最优划分属性。**

**信息增益率：** $gain_{rate}(D,A) = gain(D,A)/entropy(D,A)$

​	**tips：增益率对可能取值数目较少的属性有所偏好**

**基尼指数:** $gini(D) = \sum_{k=1}^n p_k *(1-p_k) = \sum_{k=1}^n(p_k-p_k^2) = 1-\sum_{k=1}^np_k^2$

​	其中D为数据集，$k$为数据集D的可能分类标签,$P_k$为该标签的概率

**条件基尼指数：**$gini(D,A) = \sum_{i=1}^k \frac {D_{A_i}}{D} gini(D_{A_i})$

#### ID3算法

​	以原始数据集作为决策树的根节点，应用**信息增益**准则选择特征，将当前的数据集划分为**多个子集**，作为下一层的节点，在递归地处理这些节点，直到每个节点都是空集或者只包含一个类别的样本

![1525184150012](/inner_ref/2018-04-30-Tree/1525184150012.png)

![1525184176626](/inner_ref/2018-04-30-Tree/1525184176626.png)

​	上面的图就是在西瓜数据集上用ID3算法基于信息增益生成的决策树。它具有以下特点：

> 1. ID3算法采用了**信息增益**的方式做划分选择。仔细分析一下会发现一个问题：我们需要找到$gain(D,A)$最大的特征，对于一个数据集$entropy(D)$是给定的，也就是说我们需要$entropy(D,A)$最小，意思就是我们所选的特征是那些分完后子节点的纯度最高的特征，什么样的特征分完后子节点的特征纯度比较高(熵比较小)，该特征的子类别很多，即可取值很多的这一类特征。**总结一下就是信息增益偏向于去那些拥有很多子类的特征。**这是它致命的缺点
> 2. 它只能处理那些分类的特征，对于连续值特征毫无办法（其实我们可以人为的把连续属性给离散化，但是人为必然会导致可能不准确）。
> 3. 缺失值处理：需要人工处理。比如需要单独赋值或赋一个平均值

#### C4.5 算法

​	C4.5算法是ID3算法的改进，主要改进点就是解决了ID3的几个缺点。因此它跟 ID3 算法很相似。它具有以下特点：

> 1.  采用**信息增益率**来代替ID3中的信息增益。我们提到信息增益对可取值数目较多的属性有所偏好，但是增益率对可能取值数目较少的属性有所偏好。因此，C4.5并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式选择方式：**先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。**
> 2. C4.5采用了二分法（bi-partition）处理连续特征。1）对于连续值属性的值进行排序(A1,A2......An);2）在每两个值之间取一个点，用这个点就可以把该组连续值分为两部分，将它作为一个分裂点。为了找到最优分裂点我们需要遍历所有分裂点，因此它的时间开销很大。
> 3. 对于缺失值处理C4.5给出了一个很优雅的方式：C4.5让同一个样本以不同概率划入到不同的子节点中去。

缺失值处理方式，简单来说就是给每个样本赋予一个权重，初始为1。当在进行划分属性选择的时候只使用在该属性上没有缺失的样本。在将样本划分到子节点时，根据不同属性值的概率放入子节点中

![1525186548382](/inner_ref/2018-04-30-Tree/1525186548382.png)

![1525186574830](/inner_ref/2018-04-30-Tree/1525186574830.png)

#### CART树

CART树全称：Classification and Regression Trees。从名字就可以看出分类和回归任务它都可以做。这个树非常出名，我们以后使用到的random forest，gbdt， xgboost里面的base estimator 都是CART树。

CART算法由以下两步组成：

> 1. **决策树生成**：基于训练数据集生成决策树，生成的决策树要尽量大；
> 2. **决策树剪枝**：用验证数据集对已生成的树进行剪枝并选择最优子树，这时损失函数最小作为剪枝的标准。

##### 分类问题

对于分类问题它跟ID3，C4.5基本差不多。主要有两个区别：

> 1. 使用基尼指数来确定最优划分点。基尼指数的通俗解释就是：表示一件事物的不确定性，基尼指数越大不确定性越大。我们要找基尼指数小的特征，这样的特征对于划分数据集的准确性会更高(不确定性低嘛)
> 2. 它是一个二叉树，内部结点特征的取值为“是”和“否”，左分支是取值为“是”的分支，右分支是取值为“否”的分支。（对于连续特征，选择某个使得基尼指数最小的划分点；对于离散特征，对每个特征取值$a$用是否  $x_A=a$来划分）

##### 回归问题

CART做回归用平方误差表示每个单元的损失，：$\sum_{x_i \in R_m}(y_i-f(x_i))^2$，**每个单元的最优输出就是使该单元的损失函数最小。**每个单元的最终输出可以表示为$C = avg(y_i|x_i)(x_i \in R_m)$(区间$R_m$ 上所有$x_i$ 的输出$y_i$的均值)

对于回归问题，我们面临的问题也是如何确定划分点(决策树的核心)。这里CART树的处理方式和C4.5处理连续变量的方式有点类似，即对于每个特征的取值，我们从中找一个点j，这个点j可以将该特征分为左右两部分。在划分选择时，选择满足左右子树平方误差之和最小的分割点：

![img](/inner_ref/2018-04-30-Tree/f33c7d4396da438d91e0b79c3f130ab8.jpg)

#### 说明

在使用决策树寻找每一步的最优切分点时，常用的是贪心算法，贪心算法有一个问题就是局部最优，而不是全局最优（因为从函数空间里所有的决策树中找出最优的决策树是NP-C问题 ，所以常采用启发式（Heuristic）的方法，如CART里面的优化GINI指数、剪枝、控制树的深度。 ）。所以我们一定要记住，**决策树在选择特征及切分点时考虑的是一个局部最优问题。**

#### 剪枝

#### 总结

在GBDT的迭代中，假设我们前一轮迭代得到的强学习器是f_{t-1}(x), 损失函数是L(y, f_{t-1}(x)), 我们本轮迭代的目标是找到一个CART回归树模型的弱学习器h_t(x)，让本轮的损失损失L(y, f_{t}(x) =L(y, f_{t-1}(x)+ h_t(x))最小。也就是说，本轮迭代找到决策树，要让样本的损失尽量变得更小。 

对决策树ID3，C4.5， CART树做一个总结，便于记忆

| 类型   | 使用范围   | 特征选择方式                     | 树形   | 缺失值                     | 连续特征       |
| ------ | ---------- | -------------------------------- | ------ | -------------------------- | -------------- |
| ID3    | 分类       | 信息增益                         | N叉树  | 人工处理                   | 不支持         |
| C4.5   | 分类       | 信息增益率                       | N叉树  | 以不同概率<br>划入不同节点 | 支持（二分法） |
| CART树 | 分类和回归 | 分类：基尼指数<br>回归：平方误差 | 二叉树 |                            | 支持（二分法） |


### 小结之承上启下

#### Bagging和Boosting 概念

Bagging和Boosting都是将已有的分类或回归算法通过一定方式组合起来，形成一个性能更加强大的分类器，更准确的说这是一种分类算法的组装方法。即将弱分类器组装成强分类器的方法。

首先介绍Bootstraping，即自助法：它是一种有放回的抽样方法（可能抽到重复的样本）。

##### Bagging

Bagging (bootstrap aggregating)即套袋法，其算法过程如下：

> 1. 从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）
> 2. 每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）
> 3. 对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）

##### Boosting

其主要思想是将弱分类器组装成一个强分类器。在PAC（概率近似正确）学习框架下，则一定可以将弱分类器组装成一个强分类器。

关于Boosting的两个核心问题：

> 1. 在每一轮如何改变训练数据的权值或概率分布？
>
>    通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。
>
> 2. 通过什么方式来组合弱分类器？
>
>    通过加法模型将弱分类器进行线性组合，比如AdaBoost通过加权多数表决的方式，即增大错误率小的分类器的权值，同时减小错误率较大的分类器的权值。而提升树通过拟合残差的方式逐步减小残差，将每一步生成的模型叠加得到最终模型。

##### Bagging和Boosting的区别

> 1. 样本选择上：
>
>    Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。
>
>    Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。
>
> 2. 样例权重：
>
>   Bagging：使用均匀取样，每个样例的权重相等
>
>   Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。
>
> 3. 预测函数：
>
>   Bagging：所有预测函数的权重相等。
>
>   Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。
>
> 4. 并行计算：
>
>   Bagging：各个预测函数可以并行生成
>
>   Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。

##### 总结

这两种方法都是把若干个分类器整合为一个分类器的方法，只是整合的方式不一样，最终得到不一样的效果，将不同的分类算法套入到此类算法框架中一定程度上会提高了原单一分类器的分类效果，但是也增大了计算量。

下面是将决策树与这些**算法框架**进行结合所得到的新的算法：

1）Bagging + 决策树 = 随机森林

2）AdaBoost + 决策树 = 提升树

3）Gradient Boosting + 决策树 = GBDT

### 随机森林

鉴于决策树容易过拟合的缺点，随机森林采用多个决策树的投票机制来改善决策树，我们假设随机森林使用了m棵决策树，那么就需要产生m个一定数量的样本集来训练每一棵树，如果用全样本去训练m棵决策树显然是不可取的，全样本训练忽视了局部样本的规律，对于模型的泛化能力是有害的 

随机森林是一种重要的基于Bagging的集成学习方法，可以用来做分类、回归等问题。 

随机森林包括**列采样和行采样**。随机森林流程：

> 1. 假如有N个样本，则**有放回**的随机选择N个样本(每次随机选择一个样本，然后返回继续选择)。这选择好了的N个样本用来训练一个决策树，作为决策树根节点处的样本。
> 2.  当每个样本有M个属性时，在决策树的每个节点需要分裂时，随机从这M个属性中选取出m个属性，满足条件m << M。然后从这m个属性中采用某种策略（比如说信息增益）来选择1个属性作为该节点的分裂属性。
> 3. 决策树形成过程中每个节点都要按照步骤2来分裂（很容易理解，如果下一次该节点选出来的那一个属性是刚刚其父节点分裂时用过的属性，则该节点已经达到了叶子节点，无须继续分裂了）。一直到不能够再分裂为止。注意整个决策树形成过程中没有进行剪枝。
> 4.  按照步骤1~3建立大量的决策树，这样就构成了随机森林了。

**注意：**采样之后的数据使用完全分裂的方式建立出决策树，这样决策树的某一个叶子节点要么是无法继续分裂的，要么里面的所有样本的都是指向的同一个分类。一般很多的决策树算法都一个重要的步骤——剪枝，但是这里不这样干，由于之前的两个随机采样的过程保证了随机性，所以就算**不剪枝**，也不会出现over-fitting。 

随机森林有许多优点：

> 1. 具有极高的准确率
> 2. 随机性的引入，使得随机森林不容易过拟合
> 3. 随机性的引入，使得随机森林有很好的抗噪声能力
> 4. 能处理很高维度的数据，并且不用做特征选择
> 5. 既能处理离散型数据，也能处理连续型数据，数据集无需规范化
> 6. 训练速度快，可以得到变量重要性排序
> 7. 容易实现并行化

随机森林的缺点：

> 1. 当随机森林中的决策树个数很多时，训练时需要的空间和时间会较大
> 2. 随机森林模型还有许多不好解释的地方，有点算个黑盒模型

### Adaboost

AdaBoost，是英文"Adaptive Boosting"（自适应增强）的缩写，由Yoav Freund和Robert Schapire在1995年提出。它的自适应在于：前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基本分类器。同时，在每一轮中加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数 

具体说来，整个Adaboost 迭代算法就3步：

> 1. 初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：1/N。
> 2. 训练弱分类器。具体训练过程中，如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它的权值就被降低；相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高。然后，权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。
> 3. 将各个训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。换言之，**误差率低的弱分类器在最终分类器中占的权重较大，否则较小。**

#### Adaboost算法流程

给定一个训练数据集T={(x1,y1), (x2,y2)…(xN,yN)}，其中实例![x \in \mathcal{X}](/inner_ref/2018-04-30-Tree/624cf12f420fb0f373cda9f7b216b2f3.png)，而实例空间![\mathcal{X} \subset \mathbb{R}^n](/inner_ref/2018-04-30-Tree/d01e9255365440ae709190fafc071951.png)，yi属于标记集合{-1,+1}，Adaboost的目的就是从训练数据中学习一系列弱分类器或基本分类器，然后将这些弱分类器组合成一个强分类器。

​    Adaboost的算法流程如下：

**步骤1.** 首先，初始化训练数据的权值分布。每一个训练样本最开始时都被赋予相同的权值：1/N 

![img](/inner_ref/2018-04-30-Tree/20141102234630160) 

**步骤2.** 进行多轮迭代，用m = 1,2, ..., M表示迭代的第多少轮

​	**a**. 使用具有权值分布Dm的训练数据集学习，得到基本分类器（选取让误差率最低的阈值来设计基本分类器）： 

​	![img](/inner_ref/2018-04-30-Tree/20141102234909561) 

​	**b**. 计算Gm(x)在训练数据集上的分类误差率 

​	![img](/inner_ref/2018-04-30-Tree/20141102235141318) 

​	由上述式子可知，Gm(x)在训练数据集上的**误差率**em就是被Gm(x)误分类样本的权值之和。 

​	**c**. 计算Gm(x)的系数，$\alpha_m$表示Gm(x)在最终分类器中的重要程度（目的：得到基本分类器在最终分类器中所占的权重）： 

​	![img](https://img-blog.csdn.net/20141102235307399) 

​	由上述式子可知，em <= 1/2时，am >= 0，且am随着em的减小而增大，意味着分类误差率越小的基本分类器在最终分类器中的作用越大。 

​	**d**. 更新训练数据集的权值分布（目的：得到样本的新的权值分布），用于下一轮迭代 

​	![img](/inner_ref/2018-04-30-Tree/20141103000618960) 

​	使得被基本分类器Gm(x)误分类样本的权值增大，而被正确分类样本的权值减小。就这样，通过这样的方式，AdaBoost方法能“重点关注”或“聚焦于”那些较难分的样本上。

​	其中，Zm是规范化因子，使得Dm+1成为一个概率分布：

​	![img](/inner_ref/2018-04-30-Tree/20141103000759596) 

**步骤3.** 组合各个弱分类器

![img](https://img-blog.csdn.net/20141103001101875) 

从而得到最终分类器，如下： 

![img](/inner_ref/2018-04-30-Tree/20141103001155359) 

### GBDT

GBDT有很多简称，有GBT（Gradient Boosting Tree）, GTB（Gradient Tree Boosting ）， GBRT（Gradient Boosting Regression Tree）, MART(Multiple Additive Regression Tree)，其实都是指的同一种算法。一般叫的比较多的是GBDT或MART（比如， LambdaMart中的Mart就是指的GBDT）。GBDT在业内有非常广泛的应用。它不需要对特征做归一化，而且能够自动做特征组合，可以得到特征的重要度。

GBDT分为两个版本：**残差版本**和**梯度版本**。这两个版本的思想有较明显区别。本文分开去讨论。

#### 残差版本

残差其实就是真实值和预测值之间的差值，在学习的过程中，首先学习一颗回归树，然后将“真实值-预测值”得到残差，再把残差作为一个学习目标，学习下一棵回归树，依次类推，直到残差小于某个接近0的阀值或回归树数目达到某一阀值。其核心思想是每轮通过拟合残差来降低损失函数。总的来说，第一棵树是正常的，之后所有的树的决策全是由残差来决定。 残差版本其实比较通俗易理解，而且经常在很多教材中提及。

它的一个简单流程：

我们先用一个初始值来学习一棵决策树，叶子处可以得到预测的值，以及预测之后的残差，然后后面的决策树就要基于前面决策树的残差来学习，直到预测值和真实值的残差为零。最后对于测试样本的预测值，就是前面许多棵决策树预测值的累加。 

该流程比较简单，而且通俗易懂。需要注意几点：

> 1. 这个版本的核心思路：每个回归树学习前面树的残差，并且用shrinkage把学习到的结果大步变小步，不断迭代学习。其中的代价函数是常见的**均方差。**
> 2. 其基本做法就是：先学习一个回归树，然后“真实值-预测值*shrinkage”求此时的残差，把这个残差作为目标值，学习下一个回归树，继续求残差……直到建立的回归树的数目达到一定要求或者残差能够容忍，停止学习。
> 3. 我们知道，残差是预测值和目标值的差值，这个版本是把残差作为全局最优的绝对方向来学习。
> 4. 这个版本更加适用于回归问题，线性和非线性的均可，而且在设定了阈值之后还可以有分类的功能。
> 5. **当时该版本使用残差，很难处理纯回归以外的问题。版本二中使用梯度，只要建立的代价函数能够求导，那么就可以使用版本二的GBDT算法，例如LambdaMART学习排序算法。**
> 6. Shrinkage和梯度下降法中学习步长alpha的关系。shrinkage设小了只会让学习更慢，设大了就等于没设，它适用于所有增量迭代求解问题；而Gradient的步长设小了容易陷入局部最优点，设大了容易不收敛。它仅用于用梯度下降求解。这两者其实没太大关系。

#### 梯度版本 

与残差版本把GBDT说成一个残差迭代树，认为每一棵回归树都在学习前N-1棵树的残差不同，Gradient版本把GBDT说成一个梯度迭代树，使用梯度下降法求解，认为每一棵回归树在学习前N-1棵树的梯度下降值。总的来说两者相同之处在于，都是迭代回归树，都是累加每颗树结果作为最终结果（Multiple Additive Regression Tree)，每棵树都在学习前N-1棵树尚存的不足，从总体流程和输入输出上两者是没有区别的。

两者的不同主要在于每步迭代时，是否使用Gradient作为求解方法。前者不用Gradient而是用残差—-残差是全局最优值，Gradient是局部最优方向*步长，即前者每一步都在试图让结果变成最好，后者则每步试图让结果更好一点。

两者优缺点。看起来前者更科学一点–有绝对最优方向不学，为什么舍近求远去估计一个局部最优方向呢？原因在于灵活性。**前者最大问题是，由于它依赖残差，cost function一般固定为反映残差的均方差，因此很难处理纯回归问题之外的问题。而后者求解方法为梯度下降，只要可求导的cost function都可以使用。**  跟残差版本比，梯度版本相对难理解，但是使用范围更广，非常适合跟其他模型组合，比如我们后续会讲到的LambdaMart。

**定义：**GBDT使用CART回归树模型，在迭代中，假设我们前一轮迭代得到的强学习器是$f_{t-1}(x)$, 损失函数是$L(y, f_{t-1}(x))$, 我们本轮迭代的目标是找到一个CART回归树模型的弱学习器$h_t(x)$，让本轮的损失损失$L(y, f_{t}(x)) =L(y, f_{t-1}(x)+h_t(x))$最小。也就是说，本轮迭代找到决策树，要让样本的损失尽量变得更小。 

##### GBDT的负梯度拟合（每棵树的真值）

我们介绍了GBDT的基本思路，但是没有解决损失函数拟合方法的问题。针对这个问题，大牛Freidman提出了用损失函数的负梯度来拟合本轮损失的近似值，进而拟合一个CART回归树。**第t轮的第i个样本**的损失函数的负梯度表示为

$r_{ti} = -\bigg[\frac{\partial L(y_i, f(x_i)))}{\partial f(x_i)}\bigg]_{f(x) = f_{t-1}\;\; (x)}$

利用$(x_i,r_{ti})\;\; (i=1,2,..m)​$,我们可以拟合一颗CART回归树，得到了第t颗回归树，其对应的叶节点区域$R_{tj}, j =1,2,..., J​$。其中$J​$为叶子节点的个数。（**tips：负梯度也就是每棵树拟合时的真值**）

针对每一个叶子节点里的样本，我们求出使损失函数最小，也就是拟合叶子节点最好的的输出值$c_{tj}$如下：

$c_{tj} = \underbrace{arg\; min}_{c}\sum\limits_{x_i \in R_{tj}} L(y_i,f_{t-1}(x_i) +c)$

如果损失函数式平方误差 $c_{tj}=\frac{\sum\limits_{{x_i\in R_{tj}}}(y_i-f_{t-1}(x_i))}{n}$, 也就是残差的均值

这样我们就得到了本轮的决策树拟合函数如下：

$h_t(x) = \sum\limits_{j=1}^{J}c_{tj}I(x \in R_{tj})$

从而本轮最终得到的强学习器的表达式如下：

$f_{t}(x) = f_{t-1}(x) + \sum\limits_{j=1}^{J}c_{tj}I(x \in R_{tj})$

通过损失函数的负梯度来拟合，**我们找到了一种通用的拟合损失误差的办法，这样无论是分类问题还是回归问题，我们通过其损失函数的负梯度的拟合，就可以用GBDT来解决我们的分类回归问题。区别仅仅在于损失函数不同导致的负梯度不同而已。**

GBDT常用的损失函数和对应的负梯度。

![img](/inner_ref/2018-04-30-Tree/517947-20170609154837559-1727480067.png) 

##### GBDT回归算法

好了，有了上面的思路，下面我们总结下GBDT的回归算法。为什么没有加上分类算法一起？那是因为分类算法的输出是不连续的类别值，需要一些处理才能使用负梯度，我们在下一节讲。 

输入是训练集样本$T=\{(x_,y_1),(x_2,y_2), ...(x_m,y_m)\}$， 最大迭代次数T, 损失函数L。

　　　　输出是强学习器$f(x)$

　　　　1) 初始化弱学习器

​                       $f_0(x) = \underbrace{arg\; min}_{c}\sum\limits_{i=1}^{m}L(y_i, c)$

　　　　2) 对迭代轮数$t=1,2,...T$有：

　　　　　　a)对样本$i=1,2，...m$，计算负梯度

​                              $r_{ti} = -\bigg[\frac{\partial L(y_i, f(x_i)))}{\partial f(x_i)}\bigg]_{f(x) = f_{t-1}\;\; (x)}$

　　　　　　b)利用$(x_i,r_{ti})\;\; (i=1,2,..m)$, 拟合一颗CART回归树,得到第t颗回归树，其对应的叶子节点区域为$R_{tj}, j =1,2,..., J$。其中$J$为回归树t的叶子节点的个数。

　　　　　　c) 对叶子区域$j =1,2,..J,$计算最佳拟合值

​                             $c_{tj} = \underbrace{arg\; min}_{c}\sum\limits_{x_i \in R_{tj}} L(y_i,f_{t-1}(x_i) +c)$

　　　　　　d) 更新强学习器

​                           $f_{t}(x) = f_{t-1}(x) + \sum\limits_{j=1}^{J}c_{tj}I(x \in R_{tj})$

　　　　3) 得到强学习器$f(x)$的表达式

​                         $f(x) = f_T(x) =f_0(x) + \sum\limits_{t=1}^{T}\sum\limits_{j=1}^{J}c_{tj}I(x \in R_{tj})$

截图来自《The Elements of Statistical Learning》 

![img](/inner_ref/2018-04-30-Tree/517947-20170609155109106-1915188978.png) 

##### GBDT分类算法

这里我们再看看GBDT分类算法，GBDT的分类算法从思想上和GBDT的回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合类别输出的误差。

为了解决这个问题，主要有两个方法，一个是用指数损失函数，此时GBDT退化为Adaboost算法。另一种方法是用类似于逻辑回归的对数似然损失函数的方法。也就是说，我们用的是类别的预测概率值和真实概率值的差来拟合损失。本文仅讨论用对数似然损失函数的GBDT分类。而对于对数似然损失函数，我们又有二元分类和多元分类的区别。

**二元GBDT分类算法**

对于二元GBDT，如果用类似于逻辑回归的对数似然损失函数，则损失函数为：

$L(y, f(x)) = log(1+ exp(-yf(x)))$

其中$y \in\{-1, +1\}$。则此时的负梯度误差为

$r_{ti} = -\bigg[\frac{\partial L(y, f(x_i)))}{\partial f(x_i)}\bigg]_{f(x) = f_{t-1}\;\; (x)} = y_i/(1+exp(y_if(x_i)))$

对于生成的决策树，我们各个叶子节点的最佳残差拟合值为

$c_{tj} = \underbrace{arg\; min}_{c}\sum\limits_{x_i \in R_{tj}} log(1+exp(-y_i(f_{t-1}(x_i) +c)))$

由于上式比较难优化，我们一般使用近似值代替

$c_{tj} = \sum\limits_{x_i \in R_{tj}}r_{ti}\bigg /  \sum\limits_{x_i \in R_{tj}}|r_{ti}|(1-|r_{ti}|)$

除了负梯度计算和叶子节点的最佳残差拟合的线性搜索，二元GBDT分类和GBDT回归算法过程相同



**多元GBDT分类算法**

多元GBDT要比二元GBDT复杂一些，对应的是多元逻辑回归和二元逻辑回归的复杂度差别。假设类别数为K，则此时我们的对数似然损失函数为：

$L(y, f(x)) = -  \sum\limits_{k=1}^{K}y_klog\;p_k(x)$

其中如果样本输出类别为$k$，则$y_k=1$。第$k$类的概率$p_k(x)$的表达式为：

$p_k(x) = exp(f_k(x)) \bigg / \sum\limits_{l=1}^{K} exp(f_l(x))$

集合上两式，我们可以计算出第t轮的第i个样本对应类别l的负梯度误差为
$r_{til} = -\bigg[\frac{\partial L(y_i, f(x_i)))}{\partial f(x_i)}\bigg]_{f_k(x) = f_{l, t-1}\;\; (x)} = y_{il} - p_{l, t-1}(x_i)$
观察上式可以看出，其实这里的误差就是样本i对应类别l的真实概率和$t-1$轮预测概率的差值。

对于生成的决策树，我们各个叶子节点的最佳残差拟合值为

$c_{tjl} = \underbrace{arg\; min}_{c_{jl}}\sum\limits_{i=0}^{m}\sum\limits_{k=1}^{K} L(y_k, f_{t-1, l}(x) + \sum\limits_{j=0}^{J}c_{jl} I(x_i \in R_{tj}))$

由于上式比较难优化，我们一般使用近似值代替

$c_{tjl} =  \frac{K-1}{K} \; \frac{\sum\limits_{x_i \in R_{tjl}}r_{til}}{\sum\limits_{x_i \in R_{til}}|r_{til}|(1-|r_{til}|)}$

除了负梯度计算和叶子节点的最佳残差拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同。

#### XGBoost

严格来说XGBoost是一个工具，它是GBDT的一种实现，在基本GBDT基础上它做了很多的改进，使得GBDT发扬光大。XGBoost的一些特点(知乎)

> 1. 传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。（本人不是太了解）
> 2. 传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，**xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导**。 
> 3. **xgboost在代价函数里加入了正则项，用于控制模型的复杂度。**正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。
> 4. Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）
> 5. 列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。
> 6. 对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。 
> 7. xgboost工具支持并行。boosting不是一种串行的结构吗?怎么并行的？注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。
> 8. 可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。
> 9. xgboost工具支持并行。boosting不是一种串行的结构吗?怎么并行的？注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行
> 10. 可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。

本人觉得2、3点非常重要，2使得用户可以根据需求自定义损失函数，也可以方便的跟其他模型组合。3防止了模型过拟合，相当于做了预剪枝。

下面对陈天奇的slides做一些摘录。

XGBoost的损失函数（也叫目标函数）表示为：

![1525825181426](/inner_ref/2018-04-30-Tree/1525825181426.png)

损失函数分为两部分： 第一部分控制训练误差，它跟样本数有关，第二部分控制模型复杂度，它跟树的个数有关。



在t轮迭代中，我们的未知量为$f_t(x_i)$， $\hat{y}_i^{(t-1)}$是前t-1轮的结果，是已知量。目标函数可以转换为

![1525825323967](/inner_ref/2018-04-30-Tree/1525825323967.png)

我们针对一棵树做如下定义：

$q_{(x)}$表示样本x落在了第$q_{x}$个叶子节点上。

$T$表示叶子节点总数

叶子节点j上的样本结合表示为$I_j=\{i|q(x_i)=j\}$

$w_i$表示第i个叶子节点上的取值。所以对于一个样本有：$f_t(x) = w_{q(x)}$。(**这个跟GBDT中的$f_t(x)$含义不一样**)

![1525825976283](/inner_ref/2018-04-30-Tree/1525825976283.png)

控制模型复杂度的正则项，它包括两部分： 第一部分控制叶子节点的个数，第二部分控制叶子节点值的大小。

定义如下：

![1525826188455](/inner_ref/2018-04-30-Tree/1525826188455.png)

对目标函数做泰勒展开（g表示一阶导数，h表示二阶导数）

![1525826525569](/inner_ref/2018-04-30-Tree/1525826525569.png)

![1525826637811](/inner_ref/2018-04-30-Tree/1525826637811.png)

**上面的$w^*_j$表示叶子节点的取值。**

如何构建一颗树？ XGBoost采用了贪心算法， 寻找gain最大的分裂位置

![1525826756980](/inner_ref/2018-04-30-Tree/1525826756980.png)

![1525826888663](/inner_ref/2018-04-30-Tree/1525826888663.png)

### IR的预备知识

详细参考：http://blog.sina.com.cn/s/blog_72995dcc01013oo9.html

http://lixinzhang.github.io/xin-xi-jian-suo-zhong-de-ping-jie-zhi-biao-maphe-ndcg.html

本文主要介绍下MAP和NDCG

#### MAP

MAP全称Mean Average Precision，表示平均正确率。其中AP的计算方法如下：

 $$AveP=\frac{\sum_{k=1}^{n}(P(k) \times rel(k))}{相关文档数量}$$ 

其中，$k$为检索结果队列中的排序位置，$P(k)$为前$k$个结果的准确率，即$P(k)=\frac{相关文档数量}{总文档数量}$，$$rel(k)$$表示位置$$k$$的文档是否相关，相关为1，不相关为0。

MAP即是将多个query对应的AP求平均。 $$MAP=\frac{\sum_{q=1}^{Q} AveP(q)}{Q}$$ Q为query的数量。

**举例**

假设有两个主题，主题1有4个相关网页，主题2有5个相关网页。某系统对于主题1检索出4个相关网页，其rank分别为1, 2, 4, 7；对于主题2检索出3个相关网页，其rank分别为1,3,5。对于主题1，平均准确率为(1/1+2/2+3/4+4/7)/4=0.83。对于主题2，平均准确率为(1/1+2/3+3/5+0+0)/5=0.45。则MAP= (0.83+0.45)/2=0.64。” ——[例子来源](http://www.cnblogs.com/ywl925/archive/2013/08/16/3262209.html)

#### NDCG

先说CG（Cumulative Gain，累计增益），

$$
CG_{p} = \sum_{i=1}^{p}rel_i
$$

其中，p为文档在搜索结果列表中的排序位置，$rel_i$为处在该位置文档的等级相关性（graded relevance）。

CG的劣势是等级相关性与位置无关，但这样并不合理，将一个相关性更高的结果替换排在前面相关性较弱的结果，应该更佳，但是CG的表现是两者无差异。因此，引入了DCG（Discounted Cumulative Gain）。

$$
DCG_{p}=\sum_{i=1}^{P} \frac{2^{rel_i} - 1}{log_{2}^{i+1}}
$$

DCG考虑了位置的影响，表示结果位置越靠前的文档，其相关性表现对整体排序质量的影响越大。

然而，DCG仍有一个缺点，**不同query返回的搜索结果数量不同，其DCG的值相差很大，是不可比的**。因此，需要对DCG做一定的归一化，于是有了NDCG（Normalized DCG）。

$$
NDCG_p=\frac{DCG_p}{IDCG_p}
$$

其中，$IDCG_p$为搜索结果按相关性排序之后能得到的最大DCG值。

举例

维基百科上的例子： 搜索结果为文档D1,D2,D3,D4,D5,D6，相关性分数分别为3，2，3，0，1，2，则：
$CG_6 = 3 + 2 + 3 + 0 + 1 + 2=11$
$DCG_6 = \sum_{i=1}^{6} \frac{2^{rel_i}-1}{log_2^{i+1}}=8.10$
按相关性排序可以得到最优结果，即最大DCG为文档按照{3，3，2，2，1，0}排序：
$IDCG_6=8.69$
$NDCG_6 = \frac{DCG_6}{IDCG_6}=0.932$

### RankNet

Ranknet提供了一种基于Pairwise的训练方法，它最早由微软研究院的Chris Burges等人在2005年ICML上的一篇论文Learning to Rank Using Gradient Descent中提出，并被应用在微软的搜索引擎Bing当中。 

#### 样本构造

RankNet属于pairWise， 它的样本是两两数据比较相关性得到的。比如对于一次搜索引擎搜索，召回了N条URL：$U_1,U_2,...,U_n$。样本选取任意两个URL: $U_i,U_j$，得到pair样本： （<$U_i,U_j$>, $S_{ij}$）。元组前面为两个URL的特征，$S_{ij}$取值为$\{0, +1, -1\}$。为1表示$U_i$比$U_j$更相关。

#### 相关性概率

Cost function是RankNet算法的核心，在介绍Cost function前，我们先定义两个概率：预测相关性概率、真实相关性概率。 

**预测相关性概率**  

对于任意一个URL对$(U_i,U_j)$，模型输出的score分别为$s_i$和$s_j$，那么根据模型的预测，$U_i$比$U_j$与Query**更相关的概率为**：
$$
P_{ij} = P(U_i>U_j) = {1\over {1+e^{-\sigma(s_i-s_j)}}}
$$
由于RankNet使用的模型一般为神经网络，根据经验sigmoid函数能提供一个比较好的概率评估。参数$\sigma$决定sigmoid函数的形状，对最终结果影响不大。 

**真实相关性概率** 

对于训练数据中的$U_i$和$U_j$，它们都包含有一个与Query相关性的真实label，比如$U_i$与Query的相关性label为good，$U_j$与Query的相关性label为bad，那么显然$U_i$比$U_j$更相关。我们定义$\overline p_{ij}$为$U_i$比$U_j$**更相关的真实概率**，有
$$
\overline p_{ij} = {1\over2 }(1+S_{ij})
$$
如果$U_i$比$U_j$更相关，那么$S_{ij}=1$；如果$U_i$不如$U_j$相关，那么$S_{ij}=-1$；如果$U_i$、$U_j$与Query的相关程度相同，那么$S_{ij}=0$。 

#### 代价函数

对于一个排序，RankNet从各个URL的相对关系来评价排序结果的好坏，排序的效果越好，那么有错误相对关系的pair就越少。所谓错误的相对关系即如果根据模型输出$U_i$排在$U_j$前面，但真实label为$U_i$的相关性小于$U_j$，那么就记一个错误pair，RankNet就是以错误的pair最少为优化目标。对于每一个pair，我们使用交叉熵来度量其预测代价，即： 
$$
C_{ij} = -\overline P_{ij}logP_{ij}-(1-\overline P_{ij})log(1-P_{ij})
$$
化简
$$
\begin{align}  C_{ij}  &= -{1\over2}(1+S_{ij})log{1\over 1+e^{-\sigma(s_{i}-s_{j})}}-{1\over2}(1-S_{ij})log{e^{-\sigma(s_i-s_j)}\over 1+e^{-\sigma(s_i-s_j)}} \\&=-{1\over2}(1+S_{ij})log{1\over 1+e^{-\sigma(s_{i}-s_{j})}}-{1\over2}(1-S_{ij})[-\sigma(s_i-s_j)+log{1\over 1+e^{-\sigma(s_i-s_j)}}]\\&={1\over2}(1-S_{ij})\sigma(s_i-s_j)+log(1+e^{-\sigma(s_i-s_j)})\end{align}
$$
![](/inner_ref/2018-04-30-Tree/20151024210741613) 

当$S_{ij}=1$时,我们有:

![1526297017176](/inner_ref/2018-04-30-Tree/1526297017176.png)

当$S_{ij}=-1$时:

![1526297061460](/inner_ref/2018-04-30-Tree/1526297061460.png)

该代价函数有以下特点：

> 当两个相关性不同的文档算出来的模型分数相同时，损失函数的值大于0，仍会对这对pair做惩罚，使他们的排序位置区分开
>
> 损失函数是一个类线性函数，可以有效减少异常样本数据对模型的影响，因此具有鲁棒性

总代价
$$
C = {\sum_{(i,j)\in I} }C_{ij}
$$
$I$表示所有URL pari的集合，且每个pair仅包含一次。

#### 梯度下降迭代

我们获得了一个可微的代价函数，下面我们就可以用梯度下降法来迭代更新模型参数$w_k$了，即 
$$
w_k \rightarrow w_k-\eta {\partial C \over \partial w_k}
$$
$\eta$为步长，代价C沿负梯度方向变化.

我们对${\partial C \over \partial w_k}$继续分解
$$
{\partial C \over \partial w_k}=\underset{(i,j)\in I}{\sum}( {\partial C_{ij} \over \partial s_i} {\partial s_i \over \partial w_k}+{\partial C_{ij} \over \partial s_j} {\partial s_j \over \partial w_k})
$$
其中
$$
{\partial C_{ij} \over \partial s_i} = \sigma ({1 \over 2}(1-S_{ij})-{1\over 1+e^{\sigma(s_i-s_j)}}) = -{\partial C_{ij} \over \partial s_j}
$$
我们令$\lambda_{ij} = {\partial C_{ij} \over \partial s_i} = \sigma ({1 \over 2}(1-S_{ij})-{1\over 1+e^{\sigma(s_i-s_j)}})$，有
$$
\begin{align}{\partial C \over \partial w_k} &= \underset{(i,j)\in I}{\sum}\sigma ({1 \over 2}(1-S_{ij})-{1\over 1+e^{\sigma(s_i-s_j)}})({\partial s_i \over \partial w_k}-{\partial s_j \over \partial w_k}) \\ &=\underset{(i,j)\in I}{\sum}\lambda_{ij}({\partial s_i \over \partial w_k}-{\partial s_j \over \partial w_k}) \\ &=\underset{i}{\sum}\lambda_i {\partial s_i \over \partial w_k}\end{align}
$$
下面我们来看看这个$\lambda_i$是什么。前面讲过集合$I$中只包含label不同的URL的集合，且每个pair仅包含一次，即$(U_i,U_j)$与$(U_j,U_i)$等价。为方便起见，我们假设I中只包含$(U_i,U_j)$表示$U_i$相关性大于$U_j$的pair，即I中的pair均满足$S_{ij}=1$，那么
$$
\lambda_i = \underset {j:(i,j)\in I}{\sum}\lambda_{ij} -  \underset {j:(j,i)\in I}{\sum}\lambda_{ij}
$$
这个写法是Burges的paper上的写法，我对此好久都没有理清，下面我们用一个实际的例子来看：有三个URL，其真实相关性满足$U_1>U_2>U_3$，那么集合$I$中就包含${(1,2), (1,3), (2,3)}$共三个pair 
$$
{\partial C \over \partial w_k}=(\lambda_{12}{\partial s_1 \over \partial w_k}-\lambda_{12}{\partial s_2 \over \partial w_k})+(\lambda_{13}{\partial s_1 \over \partial w_k}-\lambda_{13}{\partial s_3 \over \partial w_k})+(\lambda_{23}{\partial s_2 \over \partial w_k}-\lambda_{23}{\partial s_3 \over \partial w_k})
$$
显然$\lambda_1 =\lambda_{12}+\lambda_{13}$，$\lambda_2 =\lambda_{23}-\lambda_{12}$，$\lambda_3 =-\lambda_{13}-\lambda_{23}$，因此我所理解的$\lambda_i$应为
$$
\lambda_i = \underset {j:(i,j)\in I}{\sum}\lambda_{ij} -  \underset {k:(k,i)\in I}{\sum}\lambda_{ki}
$$
$\lambda_i$**决定着第$i$个URL在迭代中的移动方向和幅度**，比如:$S_{ij}=1$表示$U_i$比$U_j$更相关，如果$s_i$的值低于$s_j$的值(也就是模型判断错误), 此时$\lambda_{ij}$的值就是正数. 所以$\lambda_i$就应该向正梯度方向移动(也就是第i个样本的预测值应该提高)

#### LambdaRank

上面我们介绍了以错误pair最少为优化目标的RankNet算法，然而许多时候仅以错误pair数来评价排序的好坏是不够的，像NDCG或者ERR等评价指标就只关注top k个结果的排序，当我们采用RankNet算法时，往往无法以这些指标为优化目标进行迭代，以下图为例： 

![](/inner_ref/2018-04-30-Tree/20151025154133156)

图中每个线条表示一个URL，蓝色表示与Query相关的URL，灰色表示不相关的URL。下面我们用Error pair和NDCG分别来评估左右两个排序的好坏：  

**Error pair指标 :**

对于排序1，排序错误的pair共13对，故cost=13，分别为： $ (2,15)、(3,15)、(4,15)、(5,15)、(6,15)、(7,15)、(8,15)、  (9,15)、(10,15)、(11,15)、(12,15)、(13,15)、(14,15) $

对于排序2，排序错误的pair共11对，故cost=11，分别为： $ (1,4)、(2,4)、(3,4)  (1,10)、(2,10)、(3,10)、(5,10)、(6,10)、(7,10)、(8,10)、(9,10) $

所以，从Error pair角度考虑，排序2要优于排序1 

**NDCG指标** 

排序1与排序2具有相同的maxDCG@16, 
$$
maxDCG@16={{2^1-1}\over log(1+1)}+{{2^1-1}\over log(1+2)}=1.63
$$
对排序1，有 
$$
DCG@16={{2^1-1}\over log(1+1)}+{{2^1-1}\over log(1+15)}=1.25
$$

$$
NDCG@16={DCG@16 \over maxDCG@16}={1.25\over 1.63}=0.767
$$

对排序2，有 
$$
DCG@16={{2^1-1}\over log(1+4)}+{{2^1-1}\over log(1+10)}=0.72
$$

$$
NDCG@16={DCG@16 \over maxDCG@16}={0.72\over 1.63}=0.442
$$
所以，从NDCG指标来看，排序1要优于排序2。

从我们直觉感受来说, 首条的好坏要比整体偏好要更加重要, 所以排序2更优应该更合理. 而且, 图中黑色箭头表示RankNet的梯度: 他更倾向于优化排在更低位的样本, 我们其实希望的梯度是红色箭头: 让正样本尽可能的排在首位.

那么我们是否能以RankNet的思路来优化像NDCG、ERR等不连续、不平滑的指标呢？答案是肯定，我们只需稍微改动一下RankNet的$\lambda_{ij}$的定义即可  
$$
\lambda_{ij} = {-\sigma \over 1+e^{\sigma (s_i-s_j)}}|\Delta Z_{ij}|
$$
式中$\Delta Z_{ij}$表示，将$U_i$和$U_j$交换位置后，待优化指标的变化，如$\Delta NDCG$就表是将$U_i$和$U_j$进行交换，交换后排序的NDCG与交换前排序的NDCG的差值，我们把改进后的算法称之为LambdaRank。 

排序2中以箭头展示了RankNet和LambdaRank的下一轮迭代的调序方向和强度(箭头长度)，黑色箭头表示RankNet算法下$U_4$和$U_{10}$的调序方向和强度，红色箭头表示以NDCG为优化目标的LambdaRank算法下的调序方向和强度。

#### LambdaMart

 RankNet和 LambdaRank都属于一个算法框架, LambdaRank如果用GBDT实现就是LambdaMart. 从之前提到的XGBoost可以知道, 我们只要得到一阶导数G和二阶导数H就可以实现GBDT. 

树的第$i$个也叶子节点的值可以定义为: 

![1526343241317](/inner_ref/2018-04-30-Tree/1526343241317.png)

树分裂的方式可以定义为:

![1526343351694](/inner_ref/2018-04-30-Tree/1526343351694.png)

已知LambdaMart的损失函数为: 

![1526343403701](/inner_ref/2018-04-30-Tree/1526343403701.png)

**一阶导数G为:**

![1526343417553](/inner_ref/2018-04-30-Tree/1526343417553.png)

其中:

![1526343472706](/inner_ref/2018-04-30-Tree/1526343472706.png)

**二阶导数H为:**

![1526343537146](/inner_ref/2018-04-30-Tree/1526343537146.png)



**叶子节点的值为:**

![1526343592803](/inner_ref/2018-04-30-Tree/1526343592803.png)

**算法流程:**

![1526343652587](/inner_ref/2018-04-30-Tree/1526343652587.png)

至此终于完成了**树模型汇总(从决策树到随机森林, adboost, gbdt, xgboost, Lambdamart)**, 整个过程差不多2周的样子, 这个过程收货颇丰, 重新搞清楚了决策树之间的区别。深入理解了gbdt模型（工作中经常用到，但很多时候对其原理的东西考虑较少，很多地方一致很困惑，比如叶子节点的值为何这样？为何这样分裂树？如何跟其他模型结合？）。对LambdaMart也有了更深的理解，重新搞清了$\lambda_{ij}$和$\lambda_i$，RankNet跟LambdaRank的区别。

文中很多地方是从其他参考文献中摘录， 并非原创。
